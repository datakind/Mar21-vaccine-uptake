{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dimensional-egyptian",
   "metadata": {},
   "source": [
    "## EDA-01\n",
    "\n",
    "### Questions:\n",
    "1. What rules did you put in place to refine/clean the data?\n",
    "2. What number of duplicates occurred?\n",
    "3. What amount of data was nonsensical?\n",
    "4. What number of unique posts/posters occurred?\n",
    "5. Is there surprising missingness or sparsity (e.g. by time, date, location, etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "earned-engagement",
   "metadata": {},
   "source": [
    "### Field\tDescription\n",
    "\n",
    "1. **tweet_id**: Unique identifier of the tweet\n",
    "\n",
    "2. **created_at**:\tTime stamp of when the tweet was sent. \n",
    "\n",
    "3. **full_text**:\tThe text of the tweet \n",
    "\n",
    "4. **geo**\tGeo tag of the tweet. Will only be included if user has location services enabled. \n",
    "\n",
    "5. **coordinates** \tLatitude and Longitude of the tweet. Will only be included if the user has location services \n",
    "enabled. \n",
    "\n",
    "6. **place**\tTwitter identified place. Will only be included if the user has location services enabled. \n",
    "\n",
    "7. **retweet_count**\tThe number of retweets the tweet received \n",
    "\n",
    "8. **favorite_count**\tThe number of favorites the tweet received \n",
    "\n",
    "9. **possible_sensitive**\tTwitter flag if the tweet contains sensitive material. This is done by Twitter. If the tweet did not have this tagged by Twitter, we imputed NOT FOUND\n",
    "\n",
    "10. **lang**\tThe language of the tweet. We filtered towards English 'en' \n",
    "\n",
    "11. **user_id**\tThe unique identiifer of the person or entity sending the tweet\n",
    "\n",
    "### Data Collection Methodology \n",
    "The twitter public API allows for a query based keyword search to return 100 random samples of tweets containing keywords from the previous 7 days.\n",
    "\n",
    "Data was scraped from February 23 - March 3 searching for either a combination of a Covid-19 related keyword and a vaccine related key word, or just containing a hashtag keyword.\n",
    "\n",
    "The queries were randomly generated with the above logic in order to obtain unique tweets per API request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# set your cwd\n",
    "my_cwd = \"/home/mriveralanas/projects/Mar21-vaccine-uptake\"\n",
    "os.chdir(my_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data libraries\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  \n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# nlp\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import distance\n",
    "import unicodedata as ud\n",
    "from langdetect import detect \n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3094498/how-can-i-check-if-a-python-unicode-string-contains-non-western-letters\n",
    "\n",
    "latin_letters= {}\n",
    "\n",
    "def is_latin(uchr):\n",
    "    try: \n",
    "        return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "         return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "    return all(is_latin(uchr) for uchr in unistr if uchr.isalpha())\n",
    "\n",
    "def test_hashtag(unistr):    \n",
    "    unistr_strip = list(filter(lambda x: only_roman_chars(x), unistr))\n",
    "    return len(unistr_strip)>.5*len(unistr)\n",
    "\n",
    "def clean_hashtag(unistr):    \n",
    "    unistr = list(map(lambda x: x if only_roman_chars(x) else \"\", unistr))\n",
    "    return unistr\n",
    "\n",
    "#? ----- Affinity Propagation grouping\n",
    "def hashtag_group(grouping, hashtag):\n",
    "    hashtag_lower = hashtag.lower()\n",
    "    hashtag_clean = re.sub(r'[^a-zA-Z0-9]', '', hashtag_lower)\n",
    "    for key, value in grouping.items():\n",
    "        if hashtag_clean in value:\n",
    "            return key\n",
    "        elif hashtag_clean == key:\n",
    "            return key\n",
    "    # error\n",
    "    print(hashtag_clean)\n",
    "    raise ValueError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "large = \"22\"\n",
    "med = \"18\"\n",
    "small = \"16\"\n",
    "params = {\"axes.titlesize\": large, \n",
    "        \"legend.fontsize\":large,\n",
    "        \"figure.figsize\":(20,8), \n",
    "        \"axes.labelsize\":med,\n",
    "        \"axes.titlesize\":med,\n",
    "        \"xtick.labelsize\":med,\n",
    "        \"ytick.labelsize\":med,\n",
    "        \"figure.titlesize\":large\n",
    "        }\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "def plot_count_by(data, metric, groupby):\n",
    "\n",
    "    data_groupby = data[[groupby,metric]].groupby([groupby])[metric].count().reset_index()\n",
    "\n",
    "    x = data_groupby[groupby]\n",
    "    y = data_groupby[metric]\n",
    "\n",
    "    x_pos = [i for i, _ in enumerate(x)]\n",
    "    plt.bar(x_pos, y, color='green')\n",
    "    plt.xlabel(\"{} y {}\".format(metric, groupby))\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(groupby)\n",
    "\n",
    "    plt.xticks(x_pos, x)\n",
    "\n",
    "    plt.show()\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull local twitter_text\n",
    "raw_text = pd.read_csv(\"data/twitter_gdrive/twitter_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text data\n",
    "\n",
    "def convert_date(d):\n",
    "    if pd.isnull(d):\n",
    "        return \"NOT FOUND\"\n",
    "    else:\n",
    "        new_d = datetime.datetime.strptime(d, \"%a %b %d %H:%M:%S +0000 %Y\").strftime(\"%Y-%m-%d\")\n",
    "        return new_d\n",
    "\n",
    "raw_text['date'] = list(map(lambda d: convert_date(d), raw_text['created_at']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull local twitter_user\n",
    "raw_user = pd.read_csv(\"data/twitter_gdrive/twitter_user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull local twitter_hashtag\n",
    "raw_hashtag = pd.read_csv(\"data/twitter_gdrive/twitter_hashtag.csv\")\n",
    "\n",
    "# convert tweet id to string\n",
    "raw_hashtag['tweet_id'] = list(map(lambda x: str(x), raw_hashtag['tweet_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hashtag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # non english hashtags:\n",
    "# {\"中共病毒\":\"CCP Virus\", # on twitter seems to linked to anti-CCP sentiment\n",
    "#  \"上本町わたなべクリニック\":\"\",\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-president",
   "metadata": {},
   "source": [
    "### What is the Tweet location distribution over the 7 day window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_time = raw_text[['tweet_id', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_by(tweet_time, metric='tweet_id', groupby='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-assistant",
   "metadata": {},
   "source": [
    "### Tweet ID by lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_lang = raw_text[['tweet_id', 'lang']]\n",
    "raw_text_eng = raw_text_lang[list(map(lambda x: x == \"en\", raw_text_lang['lang']))]\n",
    "raw_text_otherlang = raw_text_lang[list(map(lambda x: x != \"en\", raw_text_lang['lang']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non english tweets\n",
    "plot_count_by(raw_text_otherlang, metric='tweet_id', groupby='lang')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-hampshire",
   "metadata": {},
   "source": [
    "### Analysis of Hashtags\n",
    "\n",
    "- [Difficulty with time dependent, not purely lexical relationship among hashtags](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00277)\n",
    "    - Unlike other tagging systems (e.g., Flickr1), hashtags in Twitter are used more as a symbol of community membership than for organizing a message content, as remarked in Huang, Thornton, and Efthimiadis (2010).\n",
    "    - Together, these problems reduce the effectiveness of Twitter hashtags as a means both for tracing users’ interests (owning to obscurity and sense shifts), and for capturing the worldwide impact of trending topics (because of synonymy and multilinguality).\n",
    "    - Despite this, however, better methods for analyzing the semantics of hashtags are most definitely needed, since hashtags are readily available, whereas textual analysis techniques, when applied on large and lengthy micro-blog streams, are limited both by complexity constraints and by the very reduced dimension of micro-blog texts (140 characters). What is more, real-time detection of sense-related hashtags could be used to improve the task of hashtag recommendation, thereby facilitating the monitoring of online discussions\n",
    "    - The solution proposed in this article is to use an algorithm for hashtag sense clustering based on temporal co-occurrence and similarity of the related time series, named SAX*\n",
    "    - Ferragina, Piccinno, and Santoro (2015).\n",
    "    - **The underlying idea of SAX* is that hashtags with similar temporal behavior are semantically related**\n",
    "    \n",
    "    \n",
    "    \n",
    "- [Clustering using Edit distance, lexical similarity](https://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups)\n",
    "- [Affinity Propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- cleaning\n",
    "\n",
    "# limit to english lang tweets\n",
    "hashtag_eng = raw_hashtag[list(map(lambda x: test_hashtag(x), raw_hashtag['hashtags']))]\n",
    "hashtag_noneng = raw_hashtag[list(map(lambda x: not test_hashtag(x), raw_hashtag['hashtags']))]\n",
    "\n",
    "# lower-case\n",
    "hashtag_eng_list = list(map(lambda x: x.lower(), hashtag_eng['hashtags']))\n",
    "\n",
    "#? strip non alpha numeric\n",
    "hashtag_eng_list = list(map(lambda x: re.sub(r'[^a-zA-Z0-9]', '', x), hashtag_eng_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(hashtag_eng_list)))\n",
    "len(hashtag_eng_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-obligation",
   "metadata": {},
   "source": [
    "### Run Affinity Clustering on Unique Cleaned Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "words = list(set(hashtag_eng_list))\n",
    "words = np.asarray(words)\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = AffinityPropagation(affinity=\"precomputed\", \n",
    "                              damping=0.7)\n",
    "affprop.fit(lev_similarity)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "affprop_dict = {}\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    affprop_dict[exemplar] = list(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-potter",
   "metadata": {},
   "source": [
    "### Prune Clusters:\n",
    "\n",
    "Remove members from cluster (basically create a new cluster) for members with average Lev distance less than the average of all members average pairwise lev distance to other cluster members\n",
    "\n",
    "**consider an alternative distance measure to lev distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for key, value in affprop_dict.items():\n",
    "    if len(value) <2:\n",
    "        continue\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print(\"==========================\")\n",
    "    c +=1\n",
    "    \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "affprop_dict_scrub = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(list(map(lambda v: ((v - v.min()) / (v.max() - v.min())).mean(), lev_similarity)))\n",
    "t > t.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-notification",
   "metadata": {},
   "source": [
    "## Apply to main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_eng['hashtag_group'] = list(map(lambda x: hashtag_group(grouping=affprop_dict, hashtag=x), hashtag_eng['hashtags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-violence",
   "metadata": {},
   "source": [
    "### Check that Tweets are in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_en  = raw_text[list(map(lambda x: detect(x),raw_text['full_text']))]\n",
    "\n",
    "#? add hashtag group\n",
    "raw_text_en = raw_text_eng.merge(hashtag_eng, on = \"tweet_id\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-swaziland",
   "metadata": {},
   "source": [
    "### Save to local fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-merit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

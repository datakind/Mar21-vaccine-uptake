{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>geo</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.364223e+18</td>\n",
       "      <td>2021-02-23 14:38:16+00:00</td>\n",
       "      <td>Here‚Äôs what's in the COVID relief package:\\n¬†\\nDirect checks to families\\nUnemployment assistance\\nMoney to reopen schools\\nVaccine funding\\nFood aid\\n$15 minimum wage\\nChild care funding\\nRental assistance\\nEviction moratorium\\n¬†\\nAll popular. All necessary. All urgent. Let‚Äôs get it done.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9160</td>\n",
       "      <td>38093</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>en</td>\n",
       "      <td>2.950125e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.364381e+18</td>\n",
       "      <td>2021-02-24 01:07:52+00:00</td>\n",
       "      <td>Will the National Endowment for the Arts be helping with vaccine distribution?\\n\\nWill they be producing masks suddenly?\\n\\nIf not, why are they being given $135 million in the ‚ÄúCOVID Relief Bill‚Äù?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6131</td>\n",
       "      <td>18560</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>en</td>\n",
       "      <td>1.201671e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.364610e+18</td>\n",
       "      <td>2021-02-24 16:14:15+00:00</td>\n",
       "      <td>This is both anecdotal and early, but many long covid survivors are feeling significantly better after receiving their first vaccine dose. Including me. Fascinating.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5941</td>\n",
       "      <td>63174</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>en</td>\n",
       "      <td>3.842872e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.364727e+18</td>\n",
       "      <td>2021-02-24 23:59:58+00:00</td>\n",
       "      <td>A Link to Professor Chossudovsky‚Äôs Analysis of What #Covid Is Really About\\nhttps://t.co/glJ9vnnm3o \\n\\n#covidHOAX  #PLANdemic  #Event201  #Scamdemic  #ChinaVirus  #MedicalMartialLaw  #CCPvirus  #coronavirus  #covid19  #covid_19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>2.192010e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.364727e+18</td>\n",
       "      <td>2021-02-24 23:59:58+00:00</td>\n",
       "      <td>Children warned over hugging grandparents even if they've had vaccine https://t.co/qd5pxDHlFN A small story, but it sums up the Covid debate. There is nothing resembling evidence for this prohibit, and a huge cost to two politically weak groups - the very young and the very old.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>2.868190e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51152</th>\n",
       "      <td>1.366553e+18</td>\n",
       "      <td>2021-03-02 00:55:24+00:00</td>\n",
       "      <td>EU - GERMANY ....and the beat goes on.... Where are the places in Germany to be Vaccinated? Do Germans have choice which Vaccine to select? Will Germans see a \"slack\" from CORONA(MANIA)DICTATORSHIP next Wednesday's after Merkel's \"meeting\"...? Or a mere \"talk-Show\" again? üòèüò†</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>en</td>\n",
       "      <td>7.250928e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51153</th>\n",
       "      <td>1.366549e+18</td>\n",
       "      <td>2021-03-02 00:42:14+00:00</td>\n",
       "      <td>@satyendrajain @ArvindKejriwal @DrKKAggarwal  Vaccination of frontline officers is being delayed in Power Department -Power Plants.Very few have been vaccinated while new Corona cases in power plants #Delhi Government dullness</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>en</td>\n",
       "      <td>3.052016e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51154</th>\n",
       "      <td>1.366547e+18</td>\n",
       "      <td>2021-03-02 00:31:41+00:00</td>\n",
       "      <td>Do it the Coward and Liar way. Get vaccinated and tell no one. #trump got vaccinated in January!  https://t.co/E5XLYPJ4UN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1.324768e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51155</th>\n",
       "      <td>1.366544e+18</td>\n",
       "      <td>2021-03-02 00:20:03+00:00</td>\n",
       "      <td>Trump urges supporters to get vaccinated against corona virus https://t.co/Ytq6iAbHIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>2.443156e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51156</th>\n",
       "      <td>1.366543e+18</td>\n",
       "      <td>2021-03-02 00:16:03+00:00</td>\n",
       "      <td>Courtesy of @voxdotcom, so important to realize that efficacy doesn't matter since ZERO PEOPLE have died from #corona after being vaccinated (hence 100% reduction in deaths) https://t.co/iifa0Ia0Um</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1.991185e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51157 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id                 created_at  \\\n",
       "0      1.364223e+18  2021-02-23 14:38:16+00:00   \n",
       "1      1.364381e+18  2021-02-24 01:07:52+00:00   \n",
       "2      1.364610e+18  2021-02-24 16:14:15+00:00   \n",
       "3      1.364727e+18  2021-02-24 23:59:58+00:00   \n",
       "4      1.364727e+18  2021-02-24 23:59:58+00:00   \n",
       "...             ...                        ...   \n",
       "51152  1.366553e+18  2021-03-02 00:55:24+00:00   \n",
       "51153  1.366549e+18  2021-03-02 00:42:14+00:00   \n",
       "51154  1.366547e+18  2021-03-02 00:31:41+00:00   \n",
       "51155  1.366544e+18  2021-03-02 00:20:03+00:00   \n",
       "51156  1.366543e+18  2021-03-02 00:16:03+00:00   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                full_text  \\\n",
       "0      Here‚Äôs what's in the COVID relief package:\\n¬†\\nDirect checks to families\\nUnemployment assistance\\nMoney to reopen schools\\nVaccine funding\\nFood aid\\n$15 minimum wage\\nChild care funding\\nRental assistance\\nEviction moratorium\\n¬†\\nAll popular. All necessary. All urgent. Let‚Äôs get it done.   \n",
       "1                                                                                                   Will the National Endowment for the Arts be helping with vaccine distribution?\\n\\nWill they be producing masks suddenly?\\n\\nIf not, why are they being given $135 million in the ‚ÄúCOVID Relief Bill‚Äù?   \n",
       "2                                                                                                                                   This is both anecdotal and early, but many long covid survivors are feeling significantly better after receiving their first vaccine dose. Including me. Fascinating.   \n",
       "3                                                                    A Link to Professor Chossudovsky‚Äôs Analysis of What #Covid Is Really About\\nhttps://t.co/glJ9vnnm3o \\n\\n#covidHOAX  #PLANdemic  #Event201  #Scamdemic  #ChinaVirus  #MedicalMartialLaw  #CCPvirus  #coronavirus  #covid19  #covid_19   \n",
       "4                 Children warned over hugging grandparents even if they've had vaccine https://t.co/qd5pxDHlFN A small story, but it sums up the Covid debate. There is nothing resembling evidence for this prohibit, and a huge cost to two politically weak groups - the very young and the very old.   \n",
       "...                                                                                                                                                                                                                                                                                                   ...   \n",
       "51152                 EU - GERMANY ....and the beat goes on.... Where are the places in Germany to be Vaccinated? Do Germans have choice which Vaccine to select? Will Germans see a \"slack\" from CORONA(MANIA)DICTATORSHIP next Wednesday's after Merkel's \"meeting\"...? Or a mere \"talk-Show\" again? üòèüò†   \n",
       "51153                                                                  @satyendrajain @ArvindKejriwal @DrKKAggarwal  Vaccination of frontline officers is being delayed in Power Department -Power Plants.Very few have been vaccinated while new Corona cases in power plants #Delhi Government dullness   \n",
       "51154                                                                                                                                                                           Do it the Coward and Liar way. Get vaccinated and tell no one. #trump got vaccinated in January!  https://t.co/E5XLYPJ4UN   \n",
       "51155                                                                                                                                                                                                               Trump urges supporters to get vaccinated against corona virus https://t.co/Ytq6iAbHIH   \n",
       "51156                                                                                               Courtesy of @voxdotcom, so important to realize that efficacy doesn't matter since ZERO PEOPLE have died from #corona after being vaccinated (hence 100% reduction in deaths) https://t.co/iifa0Ia0Um   \n",
       "\n",
       "       geo coordinates place  retweet_count  favorite_count  \\\n",
       "0      NaN         NaN   NaN           9160           38093   \n",
       "1      NaN         NaN   NaN           6131           18560   \n",
       "2      NaN         NaN   NaN           5941           63174   \n",
       "3      NaN         NaN   NaN              1               0   \n",
       "4      NaN         NaN   NaN              0               2   \n",
       "...    ...         ...   ...            ...             ...   \n",
       "51152  NaN         NaN   NaN              2               2   \n",
       "51153  NaN         NaN   NaN              0               0   \n",
       "51154  NaN         NaN   NaN              0               1   \n",
       "51155  NaN         NaN   NaN              8              32   \n",
       "51156  NaN         NaN   NaN              0               2   \n",
       "\n",
       "      possibly_sensitive lang       user_id  \n",
       "0              NOT FOUND   en  2.950125e+07  \n",
       "1              NOT FOUND   en  1.201671e+18  \n",
       "2              NOT FOUND   en  3.842872e+07  \n",
       "3                  False   en  2.192010e+09  \n",
       "4                  False   en  2.868190e+09  \n",
       "...                  ...  ...           ...  \n",
       "51152          NOT FOUND   en  7.250928e+17  \n",
       "51153          NOT FOUND   en  3.052016e+09  \n",
       "51154              False   en  1.324768e+18  \n",
       "51155              False   en  2.443156e+07  \n",
       "51156              False   en  1.991185e+08  \n",
       "\n",
       "[51157 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load\n",
    "df = pd.read_csv('Mar21-vaccine-uptake-main 3/data/twitter/twitter_data_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['copy_index'] = df.index\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads\n",
    "stopwords = set(w.rstrip() for w in open('stopwords/stopwords.txt'))\n",
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other method testing\n",
    "#positive = pd.read_csv('positive_emotions/positive-words.txt', delimiter = \"\\t\")\n",
    "#negative = pd.read_csv('afinn-master/afinn/data/AFINN-en-165.txt', delimiter = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text\n",
    "positive = set(w.rstrip() for w in open('positive_emotions/positive-words.txt'))\n",
    "negative = set(w.rstrip() for w in open('afinn-master/afinn/data/AFINN-en-165.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.dataquest.io/blog/natural-language-processing-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = tknzr.tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = []\n",
    "for index, i in enumerate(zip(df['copy_index'], df['full_text'])):\n",
    "    copy_index= i[0]\n",
    "    text = tokenizer(i[1])\n",
    "    \n",
    "    new_dataframe.append({'copy_index' : copy_index,\n",
    "                       'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>copy_index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[what's, covid, relief, package, direct, check, family, unemployment, assistance, money, reopen, school, vaccine, funding, food, aid, minimum, wage, child, care, funding, rental, assistance, eviction, moratorium, popular, urgent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[national, endowment, art, helping, vaccine, distribution, producing, mask, suddenly, 135, million, covid, relief, bill]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[anecdotal, covid, survivor, feeling, significantly, receiving, vaccine, dose, including, fascinating]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[link, professor, chossudovsky, analysis, #covid, https://t.co/glj9vnnm3o, #covidhoax, #plandemic, #event201, #scamdemic, #chinavirus, #medicalmartiallaw, #ccpvirus, #coronavirus, #covid19, #covid_19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[child, warned, hugging, grandparent, they've, vaccine, https://t.co/qd5pxdhlfn, story, sum, covid, debate, resembling, evidence, prohibit, huge, cost, politically, weak]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51152</th>\n",
       "      <td>51152</td>\n",
       "      <td>[germany, ..., beat, ..., germany, vaccinated, german, choice, vaccine, select, german, slack, corona, mania, dictatorship, wednesday's, merkel's, meeting, ..., mere, talk-show]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51153</th>\n",
       "      <td>51153</td>\n",
       "      <td>[@satyendrajain, @arvindkejriwal, @drkkaggarwal, vaccination, frontline, officer, delayed, power, department, power, plants.very, vaccinated, corona, power, plant, #delhi, government, dullness]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51154</th>\n",
       "      <td>51154</td>\n",
       "      <td>[coward, liar, vaccinated, tell, #trump, vaccinated, january, https://t.co/e5xlypj4un]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51155</th>\n",
       "      <td>51155</td>\n",
       "      <td>[trump, urge, supporter, vaccinated, corona, virus, https://t.co/ytq6iabhih]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51156</th>\n",
       "      <td>51156</td>\n",
       "      <td>[courtesy, @voxdotcom, realize, efficacy, doesn't, matter, zero, people, died, #corona, vaccinated, hence, 100, reduction, death, https://t.co/iifa0ia0um]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51157 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       copy_index  \\\n",
       "0               0   \n",
       "1               1   \n",
       "2               2   \n",
       "3               3   \n",
       "4               4   \n",
       "...           ...   \n",
       "51152       51152   \n",
       "51153       51153   \n",
       "51154       51154   \n",
       "51155       51155   \n",
       "51156       51156   \n",
       "\n",
       "                                                                                                                                                                                                                                        text  \n",
       "0      [what's, covid, relief, package, direct, check, family, unemployment, assistance, money, reopen, school, vaccine, funding, food, aid, minimum, wage, child, care, funding, rental, assistance, eviction, moratorium, popular, urgent]  \n",
       "1                                                                                                                   [national, endowment, art, helping, vaccine, distribution, producing, mask, suddenly, 135, million, covid, relief, bill]  \n",
       "2                                                                                                                                     [anecdotal, covid, survivor, feeling, significantly, receiving, vaccine, dose, including, fascinating]  \n",
       "3                                   [link, professor, chossudovsky, analysis, #covid, https://t.co/glj9vnnm3o, #covidhoax, #plandemic, #event201, #scamdemic, #chinavirus, #medicalmartiallaw, #ccpvirus, #coronavirus, #covid19, #covid_19]  \n",
       "4                                                                 [child, warned, hugging, grandparent, they've, vaccine, https://t.co/qd5pxdhlfn, story, sum, covid, debate, resembling, evidence, prohibit, huge, cost, politically, weak]  \n",
       "...                                                                                                                                                                                                                                      ...  \n",
       "51152                                                      [germany, ..., beat, ..., germany, vaccinated, german, choice, vaccine, select, german, slack, corona, mania, dictatorship, wednesday's, merkel's, meeting, ..., mere, talk-show]  \n",
       "51153                                      [@satyendrajain, @arvindkejriwal, @drkkaggarwal, vaccination, frontline, officer, delayed, power, department, power, plants.very, vaccinated, corona, power, plant, #delhi, government, dullness]  \n",
       "51154                                                                                                                                                 [coward, liar, vaccinated, tell, #trump, vaccinated, january, https://t.co/e5xlypj4un]  \n",
       "51155                                                                                                                                                           [trump, urge, supporter, vaccinated, corona, virus, https://t.co/ytq6iabhih]  \n",
       "51156                                                                             [courtesy, @voxdotcom, realize, efficacy, doesn't, matter, zero, people, died, #corona, vaccinated, hence, 100, reduction, death, https://t.co/iifa0ia0um]  \n",
       "\n",
       "[51157 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(new_dataframe, columns = ['copy_index', 'text'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "text = new_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenized/ Positive- Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "this_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in positive:\n",
    "    this_texts.append(i)\n",
    "    tokens = tokenizer(i)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in negative:\n",
    "    this_texts.append(i)\n",
    "    tokens = tokenizer(i)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 4494\n"
     ]
    }
   ],
   "source": [
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-b4ffa195ff0a>:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x / x.sum() # normalize it before setting label\n"
     ]
    }
   ],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-b4ffa195ff0a>:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x / x.sum() # normalize it before setting label\n"
     ]
    }
   ],
   "source": [
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "this_texts, data = shuffle(this_texts, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "#model.fit(Xtrain, Ytrain)\n",
    "#print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "#print(\"Test accuracy:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a8c489d3b79c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
